{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Car Accidents in the Contiguous United States and determining ideal driving conditions. \n## A data analysis and visualization by Montel Hardy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### The following is the cleaning, analysis, and visualization of a dataset covering nearly three years of car accident data in the United States. The below dataset features about three million accidents spanning 48 states from February 2016 to December 2019, collected by Bing and Mapquest. Below are some of the insights from the data I've visualized, many are included near their visualizations.\n\n* Three times as many accidents have occurred during the day than at night.\n* Daytime driving during inclement weather is a scenario that holds the highest risk of accidents for drivers.\n* Severe accidents aren't occuring more frequently at night than during the day, but they are more likely due to reduced visibility.\n* For more severe accidents (rating of 3 or 4), there is strong is a correlation between reduced visibility and accident severity.\n* Aside from California, states with large populations don't necessarily have a large number of accidents. Part of this could be due to the accesibility of public transportation in some states.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## References & Disclaimer\n\n\n  #### Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. “A Countrywide Traffic Accident Dataset.”, arXiv preprint arXiv:1906.05409 (2019).\n   \n #### Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. “Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.” In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019.******\n\n\n#### This dataset is being distributed only for Research purposes, under Creative Commons Attribution-Noncommercial-ShareAlike license (https://creativecommons.org/licenses/by-nc-sa/4.0/).  You may cite the above papers if you use this dataset.*******\n\n#### The dataset can be found here: https://osu.app.box.com/v/us-accidents-dec19","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"https://creativecommons.org/licenses/by-nc-sa/4.0/","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Getting Started","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###  First, I'll confirm that I'm running Python and import the proper libraries. Next, I will read in the csv file.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"run python","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport geopandas as gpd\nimport plotly.graph_objects as go\nimport matplotlib.ticker as ticker\n\n\n\nprint ('All Set!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\n\nprint('Folium installed and imported!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/2019-accident-data/US_Accidents_Dec19.csv')\n\nus_accidents = df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### First, lets inspect the dataset. By using the shape(), head(), and isnull() functions; we can get a better picture look at our dataset and identify how to best prepare the data for analysis. In the below cell, we discover that 49 columns and just under three million rows make up the shape of our data set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"us_accidents.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see a preview of the aforementioned rows and columns below. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"us_accidents.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The dataframe produced by the modified isnull() function identifies the amount of rows with missing data for each column. First, rows that have no value or have value that is unintelligible will be deleted from the dataset. As you can see below, there are a good number of columns with missing data. \n\n### We'll delete a few columns that we won't be using. This process will gave us a dataset with no missing values, a few less columns and still leave over two million accidents for analysis and visualization.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"us_accidents.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below is where I remove the columns from the dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"us_accidents.drop(['Astronomical_Twilight','Nautical_Twilight','Civil_Twilight','TMC','End_Lng','End_Lat','Number','Wind_Chill(F)',\n                  'Precipitation(in)'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"us_accidents.dropna(subset=[\"Sunrise_Sunset\",\"Description\",\"Zipcode\",\"Timezone\",\n                            \"Airport_Code\",\"Weather_Timestamp\",\"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\"Visibility(mi)\",\n                           \"Wind_Direction\",\"Weather_Condition\"], axis=0, how= 'any',inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"us_accidents.drop(['Wind_Speed(mph)'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After dropping the necessary columns and rows, our data no longer has missing values. We'll use the shape() function to ensure that we still have a very large amount of clean data to work with. With that said, the next part of this project.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"us_accidents.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"us_accidents.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### For our first visualization we'll use the US map to visualize the accident data across the 48 states. In order to do this, we have to read in a world map file and download a GeoJSON file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --quiet https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DV0101EN/labs/Data_Files/world_countries.json -O world_countries.json\n    \nprint('GeoJSON file downloaded!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_count = pd.value_counts(df['State'])\n\nfig = go.Figure(data=go.Choropleth(\n    locations=state_count.index,\n    z = state_count.values.astype(float),  \n    locationmode = 'USA-states',     \n    colorscale = 'plasma',\n    colorbar_title = \"Count\",\n))\n\nfig.update_layout(\n    title_text = 'United States Accidents Visualization (2016-2019)',\n    geo_scope='usa', \n)\n\nfig.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The above map shows us that California, Texas, and Florida have the largest amount of car accidents. After those three the totals of the rest of the high-accident states level off.\n\n### To get a better look at states with high car accident numbers, I created a top ten list. I used the groupby function to gather these numbers in a dataframe. I varied the colors to make it look more visually apealing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf_top = df.groupby('State').size().to_frame('Counts')\ndf_top = df_top.reset_index().sort_values('Counts', ascending = False)[:10]\ndf_top = df_top[::-1]   \n\ncolors = ['cyan', 'gold', 'coral', 'dodgerblue',\n     'palevioletred', 'peru', 'lightblue', 'lightsalmon','crimson', 'lawngreen',]\n\nfig, ax=plt.subplots(figsize=(15,8))\nax.barh(df_top['State'], df_top['Counts'], color = colors)\n\nfor i, (value, name) in enumerate(zip(df_top['Counts'], df_top['State'])):\n        ax.text(value, i,     name,           size=14, weight=600, ha='right', va='bottom')\n        ax.text(value, i-.25,     f'{value:,.0f}',  size=14, ha='left',  va='center')\n        \nax.text(0, 1.06, 'by State', transform=ax.transAxes, size=12, color='#777777')\nax.xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\nax.xaxis.set_ticks_position('top')\nax.tick_params(axis='x', colors='#777777', labelsize=12)\nax.set_yticks([])\nax.margins(0, 0.01)\nax.grid(which='major', axis='x', linestyle='-')\nax.set_axisbelow(True)\nax.text(0, 1.12, 'Top 10 States with the Highest Number of Accidents',\n            transform=ax.transAxes, size=24, weight=600, ha='left')\nplt.box(False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"us_accidents.columns = list(map(str, us_accidents.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"us_accidents['Sunrise_Sunset'].unique()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Next, we explore the relationship betwwen the frequency of accidents by part of day (day or night). I use the groupby() function to gather the data, then I visualize it with a bar chart.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"us_accidents.groupby('Sunrise_Sunset').size()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.groupby('Sunrise_Sunset').size().plot(kind = 'barh', \n                                  color= 'lawngreen',\n                                 align = 'center',\n                                edgecolor = 'b',\n                                 linewidth = 0.9,\n                                 width = 0.3,\n                                xerr=np.std(df.groupby('Sunrise_Sunset').size()),\n                                 grid = True,figsize=(10, 6));\n\n\nplt.title('Accidents by Time of Day', fontsize=20)\nplt.xlabel('Accidents (Millions)', fontsize=16)\nplt.ylabel('Time of Day', fontsize=16)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### These findings show us that three times as many accidents happen during the day. A limitation of the dataset we're using is that it doesn't take note of the total number of drivers on the road, but a safe assumption could be that there are a significantly larger number of drivers on the road during the day.  \n\n### In the below scatter plots, we shift gears to analyze the impact visibility has on accident severity and frequency.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sev_day= us_accidents[['Severity','Sunrise_Sunset']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\nax.set_title('Accident Severity by Visibility', fontsize=18)\nax.plot(df['Severity'], df['Visibility(mi)'], 'ko');\nplt.xlabel(\"Severity\")\nplt.ylabel('Visibility (mi)')\n\nfig, ax = plt.subplots(figsize=(10, 4),)\nax.set_title('Visibility by daypart', fontsize=18,)\nax.plot(df['Visibility(mi)'], df['Sunrise_Sunset'],'ko',color='red');\nplt.xlabel(\"Visibility (mi)\")\nplt.ylabel('Daypart')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The distribution in the first scatter plot shows us that most accidents occuring in the dataset have a severity of either two or three. The next takeaway is, more serious accidents occur more frequently if there's 80 miles or less visibility drivers.\n\n### The second plot shows that visibility is a big part of what makes night driving more dangerous, but can provide an even more dangerous scenario for daytime drivers during inclement weather. According to our data, a large number of drivers on the road with reduced visibility due inclement weather creates a high likelihood of accidents.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n\n### This notebook featured data cleaning, analysis, and visualization of a car accident dataset with well over two million entries. This was a fun, timely datasets with some insights that may be worth sharing with a friend, coworker, or family member. \n\n\n### Thanks for reading and be safe!\n\n### - Montel N. Hardy\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}